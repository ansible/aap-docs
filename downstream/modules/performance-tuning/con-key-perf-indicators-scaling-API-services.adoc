// Module file name: con-key-perf-indicators-scaling-API-services.adoc
:_mod-docs-content-type: CONCEPT
[id="key-per-indicators-scaling-API-services_{context}"]
= Key performance indicators for scaling the API services

[role="_abstract"]
Scaling adds resources to handle increased load.
This is primarily achieved through horizontal scaling (adding more pods or instances) or vertical scaling (adding CPU or memory resources to pods or instances).
Proper scaling ensures high availability and maintains performance under load.

Consider scaling your services when you observe one or more of the following key performance indicators, which suggest a component is reaching its capacity and cannot efficiently handle the current request load:

* High API latency
* High CPU utilization
* Errors that occur during periods of high traffic

== High API latency

Sustained high latency on API requests is a key performance indicator.
All requests are made through the platform {Gateway}, which acts as a proxy and forwards requests to the services in question.
The request is sent to the destination service depending on which route is in the URL of the API request:

* {Gateway}: `/api/gateway`
* {ControllerName}: `/api/controller`
* {EDAName}: `/api/eda`
* {EDAName} Event Streams: `/eda-event-streams/api/eda/v1/external_event_stream/`
* {HubName}: `/api/galaxy`

Monitoring latency on the different routes through the Envoy proxy logs enables you to identify which service requires scaling.
These routes are present in the proxy container of {Gateway} pods in {OCPShort} or in the proxy logs of the {Gateway} nodes in {VMBase} and {ContainerBase}.
Periods of time where API request latency exceeds your target thresholds (for example, 50th percentile >500ms, 99th percentile >1500ms) can indicate the need to fire alerts or scale up related web services.

== High CPU utilization

When a service's API pod shows consistently high CPU usage, it may be unable to process incoming requests in a timely manner, leading to a backlog of requests.
The following indicators suggest high CPU utilization:

* High total request time from the Envoy proxy logs with the processing time from the service's WSGI logs
* High total Envoy latency
* Requests are waiting in a queue before being processed

== Error codes

The following error codes within the proxy container of {Gateway} pods in {OCPShort} or the proxy logs of the {Gateway} nodes in {VMBase} and {ContainerBase} indicate a need to scale the service.
They are often precipitated by the services being overloaded and unable to service requests in a timely manner, and are often preceded by periods of higher latency.

* Upstream Authentication Failures: `502 UAEX` (Upstream Authentication Extension) responses in Envoy logs indicate issues during the authentication phase of a request. This suggests the authentication service is overloaded, timing out, or returning broken responses.
* Upstream Service Unhealthy: `503 UH` (Upstream Service Unhealthy) responses for a specific service mean that Envoy has marked one or more of that service's pods as unhealthy and is not sending traffic to it. This occurs when an upstream pod fails its health checks. Because health checks share the same request queue as client traffic, an overloaded pod that cannot respond to the health check in time will be temporarily removed from the load balancing pool.
* Upstream Connection Failure: `503 UF` (Upstream Connection Failure) for a specific serviceâ€™s requests indicates Envoy attempted to contact an upstream pod, but the connection failed. This can occur if the upstream service is overwhelmed and cannot accept new connections.
For more information about Envoy Response Flags (the letter codes that follow the `HTTP` response code), see link:https://www.envoyproxy.io/docs/envoy/latest/configuration/observability/access_log/usage[Access logging].