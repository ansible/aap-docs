:_mod-docs-content-type: REFERENCE
[id="ocp-a-env-a"]
= Operator {GrowthTopology}

[role="_abstract"]
The Operator-based {GrowthTopology} provides a smaller footprint deployment without redundancy for organizations getting started with {PlatformNameShort} on {OCP}. 
Included are the tested infrastructure topology, system requirements, network port configurations, and an example custom resource file for installation.

== Infrastructure topology
The Red{nbsp}Hat tested infrastructure topology for this deployment model:

.Infrastructure topology diagram
image::ocp-a-env-a.png[Operator {GrowthTopology} diagram]

[IMPORTANT]
====
While Redis and PostgreSQL can be installed as part of the {OperatorBase} process, the topology diagram represents a Red Hat supported topology where both Redis and PostgreSQL are external to {PlatformNameShort}.
====

Red Hat tests a Single Node OpenShift (SNO) cluster with these requirements: 32 GB RAM, 16 CPUs, 128 GB local disk, and 3000 IOPS.

.Infrastructure topology components
[options="header"]
|====
| Count | Component 
| 1 | {ControllerNameStart} web pod
| 1 | {ControllerNameStart} task pod
| 1 | {HubNameStart} web pod
| 1 | {HubNameStart} API pod 
| 2 | {HubNameStart} content pod
| 2 | {HubNameStart} worker pod
| 1 | {HubNameStart} Redis pod
| 1 | {EDAName} API pod
| 1 | {EDAName} activation worker pod
| 1 | {EDAName} default worker pod
| 1 | {EDAName} event stream pod
| 1 | {EDAName} scheduler pod
| 1 | {GatewayStart} pod
| 1 | Database pod
| 1 | Redis pod
|====

[NOTE]
====
You can deploy multiple isolated instances of {PlatformNameShort} into the same {OCP} cluster. To do this, use a namespace-scoped deployment model (isolated within a namespace).

This approach allows you to use the same cluster for several deployments.
====

== Tested system configurations

Red{nbsp}Hat has tested these configurations to install and run {PlatformName}:

.Tested system configurations
[options="header"]
|====
| Type | Description | Notes
| Subscription | Valid {PlatformName} subscription |
| Red Hat OpenShift  
a| 
* `Version`: 4.14
* `num_of_control_nodes`: 1
* `num_of_worker_nodes`: 1 
|
| Ansible-core | Ansible-core version {CoreUseVers} or later |
| Browser | A currently supported version of Mozilla Firefox or Google Chrome. |
| Database 
a| 
* For {PlatformNameShort} managed databases: {PostgresVers}
* For customer provided (external) databases: {PostgresVers}, 16, or 17.
a| 
* External (customer supported) databases require ICU support.
* External databases using PostgreSQL 16 or 17 must rely on external backup and restore processes. Backup and restore functionality depends on utilities provided with {PostgresVers}.
|====

== Example custom resource file 

Use this example custom resource (CR) to add your {PlatformNameShort} instance to your project:

----
apiVersion: aap.ansible.com/v1alpha1
kind: AnsibleAutomationPlatform
metadata:
  name: <aap instance name>
spec:
  eda:
    automation_server_ssl_verify: 'no'
  hub:
    storage_type: 's3'
    object_storage_s3_secret: '<name of the Secret resource holding s3 configuration>'
----

== Nonfunctional requirements

{PlatformNameShort}'s performance characteristics and capacity depend on its resource allocation and configuration. 
With OpenShift, each {PlatformNameShort} component deploys as a pod. 
You can specify resource requests and limits for each pod. 

Use the {PlatformNameShort} Custom Resource (CR) to configure resource allocation for OpenShift installations. Each configurable item has default settings. These settings are the minimum requirements for an installation, but might not meet your production workload needs. 

By default, each component's deployments use minimum resource requests but no resource limits. 
OpenShift only schedules pods with available resource requests, but the pods can consume unlimited RAM or CPU as long as the OpenShift worker node itself is not under node pressure.

In the Operator {GrowthTopology}, {PlatformNameShort} runs on a Single Node OpenShift (SNO) with 32 GB RAM, 16 CPUs, 128 GB local disk, and 3000 IOPS. 
This is not a shared environment, so {PlatformNameShort} pods have full access to all of the compute resources of the OpenShift SNO. 
In this scenario, the capacity calculation for {ControllerName} task pods comes from the underlying {OCPShort} node that runs the pod. 
It does not have access to the entire node. 
This capacity calculation influences how many concurrent jobs {ControllerName} can run. 

OpenShift manages storage distinctly from VMs. This impacts how {HubName} stores its artifacts. In the Operator {GrowthTopology}, the topology uses S3 storage because {HubName} requires a `ReadWriteMany` type storage, which is not a default storage type in OpenShift.

== Network ports

{PlatformName} uses several ports to communicate with its services. 
These ports must be open and available for {PlatformName} to work. 
Ensure that these ports are available and are not blocked by a firewall.

.Network ports and protocols
[options="header"]
|====
| Port number | Protocol | Service | Source | Destination
| 80/443 | HTTP/HTTPS | Receptor | Execution node | {OCPShort} ingress
| 80/443 | HTTP/HTTPS | Receptor | Hop node | {OCPShort} ingress
| 80/443 | HTTP/HTTPS | Platform | Customer clients | {OCPShort} ingress
| 27199 | TCP | Receptor | {OCPShort} cluster | Execution node
| 27199 | TCP | Receptor | {OCPShort} cluster | Hop node
|====
