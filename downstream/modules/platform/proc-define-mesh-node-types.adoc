:_mod-docs-content-type: PROCEDURE

[id="proc-define-mesh-node-types"]

= Add execution or hop nodes to scale your infrastructure

[role="_abstract"]
To expand job capacity, create a standalone *execution node* that can be added to run alongside a deployment of {ControllerName}.
These execution nodes are not part of the {ControllerName} Kubernetes cluster.

The control nodes run in the cluster connect and submit work to the execution nodes through Receptor.

These execution nodes are registered in {ControllerName} as type `execution` instances, meaning they are only used to run jobs, not dispatch work or handle web requests as control nodes do.

[IMPORTANT]
====
When creating an execution node, make sure the system time zone on the execution node matches `settings.TIME_ZONE` (default is 'UTC') on {ControllerName}. Fact caching relies on comparing modified times of artifact files, and these modified times are not time zone-aware. Therefore, it is critical that the timezones of the execution nodes match {ControllerName}'s time zone setting.
====

*Hop nodes* can be added to sit between the control plane of {ControllerName} and standalone execution nodes.
These hop nodes are not part of the Kubernetes cluster and are registered in {ControllerName} as an instance of type `hop`, meaning they only handle inbound and outbound traffic for otherwise unreachable nodes in different or more strict networks.

The following procedure demonstrates how to set the node type for the hosts.

ifdef::operator-mesh[]
[NOTE]
====
By default, {SaaSonAWS} includes two hop nodes that you can peer execution nodes to.
====
endif::operator-mesh[]

.Procedure
//[ddacosta]Removed specified panel to simplify changes in the future.
. From the navigation panel, select {MenuInfrastructureInstances}.
. On the *Instances* list page, click btn:[Add instance].
The *Add Instance* window opens.
+
image::instances_create_new.png[Create new Instance]
+
An instance requires the following attributes:

* *Host name*: (required) Enter a fully qualified domain name (public DNS) or IP address for your instance. This field is equivalent to `hostname` for installer-based deployments.
+
[NOTE]
====
If the instance uses private DNS that cannot be resolved from the control cluster, DNS lookup routing fails, and the generated SSL/TLS certificates is invalid.
Use the IP address instead.
====
+
* Optional: *Description*: Enter a description for the instance.
* *Instance state*: This field is auto-populated, indicating that it is being installed, and cannot be modified.
* *Listener port*: This port is used for the receptor to listen on for incoming connections.
You can set the port to one that is appropriate for your configuration.
This field is equivalent to `listener_port` in the API.
The default value is 27199, though you can set your own port value.
* *Instance type*: Only `execution` and `hop` nodes can be created.
Operator based deployments do not support Control or Hybrid nodes.
+
Options:

** *Enable instance*: Check this box to make it available for jobs to run on an execution node.
** Check the *Managed by policy* box to enable policy to dictate how the instance is assigned.
** *Peers from control nodes*:
*** If you are configuring a hop node:
**** If the hop node needs to have requests pushed directly from {ControllerName}, then check the *Peers from Control* box.
// This creates a direct communication link between the hop node and {ControllerName}.
**** If the hop node is peered to another hop node, then make sure *Peers from Control* is not checked.
*** If you are configuring an execution node:
**** If the execution node needs to have requests pushed directly from {ControllerName}, then check the *Peers from Control* box.
// This creates a direct communication link between the execution node and {ControllerName}.
**** If the execution node is peered to a hop node, then make sure that *Peers from Control* is not checked.
. Click btn:[Associate peers].
//+
//image::instances_create_details.png[Create Instance details]
ifdef::operator-mesh[]
. To verify peering configuration and the direction of traffic, you can use the topology view to view a graphical representation of your updated topology.
This can help to determine where your firewall rules might need to be updated.
For more information, see link:{URLControllerUserGuide}/assembly-controller-topology-viewer[Topology view].
endif::operator-mesh[]
ifdef::controller-UG[]
. To view a graphical representation of your updated topology, see
xref:assembly-controller-topology-viewer[Topology view].
endif::controller-UG[]
+
[NOTE]
====
Complete the following steps from any computer that has SSH access to the newly created instance.
====

. Click the image:download.png[Download,15,15] icon next to *Download Bundle* to download the tar file that includes this new instance and the files necessary to install the created node into the {AutomationMesh}.
//+
//image::instances_install_bundle.png[Install instance]
+
The install bundle has TLS certificates and keys, a certificate authority, and a proper Receptor configuration file.
+
----
receptor-ca.crt
work-public-key.pem
receptor.key
install_receptor.yml
inventory.yml
group_vars/all.yml
requirements.yml
----
+
. Extract the downloaded `tar.gz` Install Bundle from the location where you downloaded it.
To ensure that these files are in the correct location on the remote machine, the install bundle includes the `install_receptor.yml` playbook.
+
. Before running the `ansible-playbook` command, edit the following fields in the `inventory.yml` file:
+
----
all:
  hosts:
    remote-execution:
      ansible_host: localhost # change to the mesh node host name
          ansible_user: <username> # user provided
          ansible_ssh_private_key_file: ~/.ssh/<id_rsa>
----
+
* Ensure `ansible_host` is set to the IP address or DNS of the node.
* Set `ansible_user` to the username running the installation.
* Set `ansible_ssh_private_key_file` to contain the filename of the private key used to connect to the instance.
* The content of the `inventory.yml` file serves as a template and has variables for roles that are applied during the installation and configuration of a receptor node in a mesh topology.
You can change some of the other fields, or replace the file in its entirety for advanced scenarios.
For more information, see link:https://github.com/ansible/receptor-collection/blob/main/README.md[Role Variables].
. For a node that uses a private DNS, add the following line to `inventory.yml`:
+
----
 ansible_ssh_common_args: <your ssh ProxyCommand setting>
----
+
This instructs the `install-receptor.yml` playbook to use the proxy command to connect through the local DNS node to the private node.
+
. When the attributes are configured, click btn:[Save].
The *Details* page of the created instance opens.
+
. Save the file to continue.
. The system that is going to run the install bundle to setup the remote node and run `ansible-playbook` requires the `ansible.receptor` collection to be installed:
+
----
ansible-galaxy collection install ansible.receptor
----
+
or
+
----
ansible-galaxy install -r requirements.yml
----
+
* Installing the receptor collection dependency from the `requirements.yml` file consistently retrieves the receptor version specified there.
Additionally, it retrieves any other collection dependencies that might be needed in the future.
* Install the receptor collection on all nodes where your playbook will run, otherwise an error occurs.

. If `receptor_listener_port` is defined, the machine also requires an available open port on which to establish inbound TCP connections, for example, 27199.
Run the following command to open port 27199 for receptor communication (Make sure you have port 27199 open in your firewall):
+
----
sudo firewall-cmd --permanent --zone=public --add-port=27199/tcp
----
+
. Run the following playbook on the machine where you want to update your automation mesh:
+
----
ansible-playbook -i inventory.yml install_receptor.yml
----
+
[NOTE]
====
OpenSSL is required for this playbook. You can install it by running the following command: 
----
openssl -v 
----
If it returns then a version OpenSSL is installed. Otherwise you need to install OpenSSL with:
----
sudo dnf install -y openssl
----
====
+
After this playbook runs, your automation mesh is configured.
+
image::instances_list_view2.png[Instances list view]
+
[NOTE]
====
It might be the case that some servers do not listen on the receptor port (the default is 27199)

Suppose you have a Control plane with nodes A, B, and C

The following is a peering set up for three controller nodes:

Controller node A -> Controller node B

Controller node A -> Controller node C

Controller node B -> Controller node C

You can force the listener by setting

`receptor_listener=True`

However, a connection Controller B -> A is likely to be rejected as that connection already exists.

This means that nothing connects to Controller A as Controller A is creating the connections to the other nodes, and the following command does not return anything on Controller A:

`[root@controller1 ~]# ss -ntlp | grep 27199 [root@controller1 ~]#`

The RPM installer creates a strongly connected peering between the control plane nodes with a least privileged approach and opens the TCP listener only on those nodes where it is required. All the receptor connections are bidirectional, so when the connection is created, the receptor can communicate in both directions. 
====
+
ifdef::controller-UG[]
To remove an instance from the mesh, see link:https://docs.redhat.com/en/documentation/red_hat_ansible_automation_platform/2.6/html/using_automation_execution/assembly-controller-instances#ref-removing-instances[Removing instances].
endif::controller-UG[]

ifdef::operator-mesh[]
+
To remove an instance from the mesh, see link:{URLOperatorMesh}/assembly-automation-mesh-operator-aap#ref-removing-instances[Removing instances].
endif::operator-mesh[]

