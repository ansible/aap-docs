:_mod-docs-content-type: CONCEPT

[id="con-about-lightspeed-intelligent-assistant"]

= Overview

[role="_abstract"]

You can install and use {AAPchatbot} on {PlatformNameShort} {PlatformVers} on {OCPShort}.  {AAPchatbot} is an intuitive chat interface embedded within the {PlatformNameShort}, using generative artificial intelligence (AI) to answer questions about the {PlatformNameShort}. 

The {AAPchatbot} interacts with users in their natural language prompts in English, and uses Large Language Models (LLMs) to generate quick, accurate, and personalized responses. These responses empower Ansible users to work more efficiently, thereby improving productivity and the overall quality of their work. 

{AAPchatbot} requires the following configurations:

* Installation of {PlatformNameShort} {PlatformVers} on {OCP}
* Deployment of an LLM provider served by either a Red Hat AI platform or a third-party AI platform. To know the LLM providers that you can use, see xref:#LLMproviders[LLM providers]. 

[IMPORTANT]
====
Red Hat does not collect any telemetry data from your interactions with the {AAPchatbot}. 
====

Upgrading from {PlatformNameShort} 2.5 to 2.6.1 or 2.6 to 2.6.1 enables HTTPS and TLS by default for internal communication between the Ansible Lightspeed API and the {AAPchatbot} pod. Following the upgrade to {PlatformNameShort} 2.6.1, the intelligent assistant will be unavailable for approximately 60 seconds while its pod restarts.

== Integration with MCP server
{AAPchatbot} integration with the Model Context Protocol (MCP) server is available as a Technology Preview release. This integration enhances the user experience by delivering relevant, dynamically sourced data results to your queries. 

MCP is an open protocol that standardizes how applications provide context to LLMs. Using the protocol, an MCP server provides a standardized way for an LLM to increase context by requesting and receiving real-time information from external resources. The integration with an MCP server enables the {AAPchatbot} to offer an enhanced user experience by delivering relevant, dynamically sourced data results to your queries. You can configure a MCP server in the chatbot configuration secret. For more information, see link:{BaseURL}/red_hat_ansible_automation_platform/{PlatformVers}/html-single/installing_on_openshift_container_platform/index#proc-create-chatbot-config-secret_deploying-chatbot-operator[Creating a chatbot configuration secret].

[NOTE]
====
Technology Preview features are not supported with Red Hat production service level agreements (SLAs) and might not be functionally complete. Red Hat does not recommend using them in production. These features provide early access to upcoming product features, enabling customers to test functionality and provide feedback during the development process. For more information about the support scope of Red Hat Technology Preview features, see link:https://access.redhat.com/support/offerings/techpreview/[Technology Preview Features Support Scope].
====

== Ansible Automation Platform 2.6 requirements

* You have installed {PlatformNameShort} {PlatformVers} on your {OCPShort} environment. 
* You have administrator privileges for the {PlatformNameShort}.
* You have provisioned an OpenShift cluster with Operator Lifecycle Management installed.

[#LLMproviders]
== Large Language Model (LLM) provider requirements

You must have configured an LLM provider that you will use before deploying the {AAPchatbot}. 

An LLM is a type of machine learning model that can interpret and generate human-like language. When an LLM is used with the {AAPchatbot}, the LLM can interpret questions accurately and provide helpful answers in a conversational manner.

{AAPchatbot} can rely on the following LLM providers:

* *Red Hat LLM providers:*

** *{RHELAI}*
+
You can configure {RHELAI} as the LLM provider. As the {RHEL} is in a different environment than the Ansible Lightspeed deployment, the model deployment must allow access using a secure connection. For more information, see link:https://docs.redhat.com/en/documentation/red_hat_enterprise_linux_ai/1.5#creating_secure_endpoint[Optional: Allowing access to a model from a secure endpoint]. 
+
{AAPchatbot} supports vLLM Server. When self-hosting an LLM with {RHELAI}, you can use vLLM Server as the inference engine.

** *{OCPAI}*
+
You must deploy an LLM on the {OCPAI} single-model serving platform that uses the Virtual Large Language Model (vLLM) runtime. If the model deployment resides in a different OpenShift environment than the Ansible Lightspeed deployment, include a route to expose the model deployment outside the cluster. For more information, see link:https://docs.redhat.com/en/documentation/red_hat_openshift_ai_self-managed/2.23#about-the-single-model-serving-platform_serving-large-models[About the single-model serving platform].
+
{AAPchatbot} supports vLLM Server. When self-hosting an LLM with {OCPAI}, you can use vLLM Server as the inference engine.
+
[NOTE]
====
For configurations with {RHELAI} or {OCPAI}, you must host your own LLM provider instead of using a SaaS LLM provider. 
====

** *Red Hat AI Inference Server*
+
You can deploy an LLM using Red Hat AI Inference Server as your inference runtime. Red Hat AI Inference Server supports vLLM runtimes for efficient model serving and can be configured to work with Ansible Lightspeed intelligent assistant. For more information, see link:http://docs.redhat.com/en/documentation/red_hat_ai_inference_server/3.2/html/getting_started/rhaiis-getting-started-overview_getting-started[Red Hat AI Inference Server documentation].
+
If the Red Hat AI Inference Server deployment is in a different environment than the Ansible Lightspeed deployment, ensure the model deployment allows access using a secure connection and configure appropriate network routing. 
+
{AAPchatbot} supports vLLM Server when self-hosting an LLM with Red Hat AI Inference Server as the inference engine.

* *Third-party LLM providers:*

** *{OpenAI}*
+
To use {OpenAI} with the {AAPchatbot}, you need access to the link:https://openai.com/api/[{OpenAI} API platform].

** *{AzureOpenAI}*
+
To use {Azure} with the {AAPchatbot}, you need access to link:https://azure.microsoft.com/en-us/products/ai-foundry/models/openai/[{AzureOpenAI}] product page. 


== Process for configuring and using the Ansible Lightspeed intelligent assistant
Perform the following tasks to set up and use the {AAPchatbot} in your {PlatformNameShort} instance on the {OCPShort} environment:

[%header,cols="35%,65%"]
|====
| Task 
| Description

|Deploy the {AAPchatbot} on {OCPShort}
a|An {PlatformNameShort} administrator who wants to deploy the {AAPchatbot} for all Ansible users in the organization.

Perform the following tasks:

. link:{BaseURL}/red_hat_ansible_automation_platform/{PlatformVers}/html-single/installing_on_openshift_container_platform/index#proc-create-chatbot-config-secret_deploying-chatbot-operator[Create a chatbot configuration secret]. 
. link:{BaseURL}/red_hat_ansible_automation_platform/{PlatformVers}/html-single/installing_on_openshift_container_platform/index#proc-update-aap-operator-chatbot_deploying-chatbot-operator[Update the YAML file of the {PlatformNameShort} to use the chatbot connection secret].
. Optional: link:{BaseURL}/red_hat_ansible_automation_platform/{PlatformVers}/html-single/installing_on_openshift_container_platform/index#proc-change-llm-model_deploying-chatbot-operator[Change your LLM model] if you want to use a different LLM provider after deploying {LightspeedShortName}. 

| Access and use the {AAPchatbot}
| All Ansible users who want to use the intelligent assistant to get answers to their questions about the {PlatformNameShort}. For more details, see link:{BaseURL}/red_hat_ansible_automation_platform/{PlatformVers}/html-single/installing_on_openshift_container_platform/index#con-using-chatbot_deploying-chatbot-operator[Using the {AAPchatbot}].
|====
