:_mod-docs-content-type: CONCEPT

[id="con-about-lightspeed-intelligent-assistant_{context}"]

= Overview

[role="_abstract"]

The {AAPchatbot} is available on {PlatformNameShort} {PlatformVers} on {OCPShort} as a Technology Preview release. It is an intuitive chat interface embedded within the {PlatformNameShort}, using generative artificial intelligence (AI) to answer questions about the {PlatformNameShort}. 

The {AAPchatbot} interacts with users in their natural language prompts in English, and uses Large Language Models (LLMs) to generate quick, accurate, and personalized responses. These responses empower Ansible users to work more efficiently, thereby improving productivity and the overall quality of their work. 

{AAPchatbot} requires the following configurations:

* Installation of {PlatformNameShort} {PlatformVers} on {OCP}
* Deployment of an LLM served by either a Red Hat AI platform or a third-party AI platform. To know the LLM providers that you can use, see xref:#LLMproviders[LLM providers]. 

[IMPORTANT]
====
* Red Hat does not collect any telemetry data from your interactions with the {AAPchatbot}. 
* {AAPchatbot} is available as a Technology Preview feature only.
+
Technology Preview features are not supported with Red Hat production service level agreements (SLAs) and might not be functionally complete. Red Hat does not recommend using them in production. These features provide early access to upcoming product features, enabling customers to test functionality and provide feedback during the development process.
+
For more information about the support scope of Red Hat Technology Preview features, see  link:https://access.redhat.com/support/offerings/techpreview[Technology Preview Features Support Scope].
====

Upgrading from {PlatformNameShort} 2.5 to 2.6.1 or 2.6 to 2.6.1 enables HTTPS and TLS by default for internal communication between the Ansible Lightspeed API and the {AAPchatbot} pod. Following the upgrade to {PlatformNameShort} 2.6.1, the intelligent assistant will be unavailable for approximately 60 seconds while its pod restarts.

== Integration with MCP server
{AAPchatbot} integration with the Model Context Protocol (MCP) server is available as a Technology Preview release. This integration enhances the user experience by delivering relevant, dynamically sourced data results to your queries. 

MCP is an open protocol that standardizes how applications provide context to LLMs. Using the protocol, an MCP server provides a standardized way for an LLM to increase context by requesting and receiving real-time information from external resources. The integration with an MCP server enables the {AAPchatbot} to offer an enhanced user experience by delivering relevant, dynamically sourced data results to your queries. You can configure a MCP server in the chatbot configuration secret. For more information, see link:{BaseURL}/red_hat_ansible_automation_platform/{PlatformVers}/html-single/installing_on_openshift_container_platform/index#proc-create-chatbot-config-secret_deploying-chatbot-operator[Creating a chatbot configuration secret].

[NOTE]
====
Technology Preview features are not supported with Red Hat production service level agreements (SLAs) and might not be functionally complete. Red Hat does not recommend using them in production. These features provide early access to upcoming product features, enabling customers to test functionality and provide feedback during the development process. For more information about the support scope of Red Hat Technology Preview features, see link:https://access.redhat.com/support/offerings/techpreview/[Technology Preview Features Support Scope].
====

== {PlatformNameShort} requirements

* You have installed {PlatformNameShort} {PlatformVers} on your {OCPShort} environment. 
* You have administrator privileges for the {PlatformNameShort}.
* You have provisioned an OpenShift cluster with Operator Lifecycle Management installed.

[#LLMproviders]
= Large Language Model (LLM) provider requirements

You must have configured an LLM provider that you will use before deploying the {AAPchatbot}. 

An LLM is a type of machine learning model that can interpret and generate human-like language. When an LLM is used with the {AAPchatbot}, the LLM can interpret questions accurately and provide helpful answers in a conversational manner.

As part of the Technology Preview release, {AAPchatbot} can rely on the following Software as a Service (SaaS) LLM providers:

*Red Hat LLM providers*

* {RHELAI}
+
{RHELAI} is OpenAI API-compatible and is configured in a similar manner to the OpenAI provider. You can configure {RHELAI} as the LLM provider. For more information, see the link:https://www.redhat.com/en/products/ai/enterprise-linux-ai[{RHELAI}] product page.

* {OCPAI}
+
{OCPAI} is OpenAI API-compatible and is configured in a similar manner to the {OpenAI} provider. You can configure {OCPAI} as the LLM provider. For more information, see the link:https://www.redhat.com/en/products/ai/openshift-ai[{OCPAI}] product page.

[NOTE]
====
For configurations with {RHELAI} or {OCPAI}, you must host your own LLM provider instead of using a SaaS LLM provider. 
====

*Third-party LLM providers*

* {IBMwatsonxai}
+
To use IBM watsonx with the {AAPchatbot}, you need an account with link:https://www.ibm.com/products/watsonx-ai[{IBMwatsonxai}].

* {OpenAI}
+
To use {OpenAI} with the {AAPchatbot}, you need access the link:https://openai.com/api/[{OpenAI} API platform].

* {AzureOpenAI}
+
To use Microsoft Azure with the {AAPchatbot}, you need access to link:https://azure.microsoft.com/en-us/products/ai-services/openai-service[{AzureOpenAI}]. 
+
[NOTE]
====
Many self-hosted or self-managed model servers claim API compatibility with {OpenAI}. It is possible to configure the {AAPchatbot} {OpenAI} provider to point to an API-compatible model server. If the model server is truly API-compatible, especially with respect to authentication, then it might work. These configurations have not been tested by Red Hat, and issues related to their use are outside the scope of Technology Preview support.
====

= Process for configuring and using the {AAPchatbot}
Perform the following tasks to set up and use the {AAPchatbot} in your {PlatformNameShort} instance on the {OCPShort} environment:

[%header,cols="35%,65%"]
|====
| Task 
| Description

|Deploy the {AAPchatbot} on {OCPShort}
a|An {PlatformNameShort} administrator who wants to deploy the {AAPchatbot} for all Ansible users in the organization.

Perform the following tasks:

. link:{BaseURL}/red_hat_ansible_automation_platform/{PlatformVers}/html-single/installing_on_openshift_container_platform/index#proc-create-chatbot-config-secret_deploying-chatbot-operator[Create a chatbot configuration secret]. 
. link:{BaseURL}/red_hat_ansible_automation_platform/{PlatformVers}/html-single/installing_on_openshift_container_platform/index#proc-update-aap-operator-chatbot_deploying-chatbot-operator[Update the YAML file of the {PlatformNameShort} to use the chatbot connection secret].
. Optional: link:{BaseURL}/red_hat_ansible_automation_platform/{PlatformVers}/html-single/installing_on_openshift_container_platform/index#proc-change-llm-model_deploying-chatbot-operator[Change your LLM model] if you want to use a different LLM provider after deploying {LightspeedShortName}. 

| Access and use the {AAPchatbot}
| All Ansible users who want to use the intelligent assistant to get answers to their questions about the {PlatformNameShort}. For more details, see link:{BaseURL}/red_hat_ansible_automation_platform/{PlatformVers}/html-single/installing_on_openshift_container_platform/index#con-using-chatbot_deploying-chatbot-operator[Using the {AAPchatbot}].
|====
