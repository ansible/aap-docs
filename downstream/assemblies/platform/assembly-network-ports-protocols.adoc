
[id="ref-network-ports-protocols_{context}"]

= Network ports and protocols

[role="_abstract"]

{PlatformName} uses several ports to communicate with its services. These ports must be open and available for incoming connections to the {PlatformName} server in order for it to work. Ensure that these ports are available and are not blocked by the server firewall.

The following architectural diagram is an example of a fully deployed {PlatformNameShort} with all possible components.

[NOTE]
====
In some of the following use cases, hop nodes are used instead of a direct link from an execution node. Hop nodes are an option for connecting control and execution nodes. Hop nodes use minimal CPU and memory, so vertically scaling hop nodes does not impact system capacity.
====

.{PlatformNameShort} Network ports and protocols 
image::aap-network-ports-protocols.png[Interaction of Ansible Automation Platform components on the network with information about the ports and protocols that are used.]

The following table indicates the destination port and the direction of network traffic:

[NOTE]
The following default destination ports and installer inventory listed are configurable. If you choose to configure them to suit your environment, you might experience a change in behavior.

.Network ports and protocols
[cols="12%,12%,17%,17%,20%,27%,27%",options="header",]
|===
| Port | Protocol | Service | Source | Destination | Required for | Installer Inventory Variable 
| 22 | TCP | SSH | Installer node | {HubNameStart} | Installation (temporary) | `ansible_port`
| 22 | TCP | SSH | Installer node | Controller node | Installation (temporary) | `ansible_port`
| 22 | TCP | SSH | Installer node | {EDAName} node | Installation (temporary) | `ansible_port`
| 22 | TCP | SSH | Installer node | Execution node | Installation (temporary) | `ansible_port`
| 22 | TCP | SSH | Installer node | Hop node | Installation (temporary) | `ansible_port`
| 22 | TCP | SSH | Installer node | Hybrid node | Installation (temporary) | `ansible_port`
| 22 | TCP | SSH | Installer node | PostgreSQL database| Remote access during installation (temporary) | `pg_port`
| 80/443 | TCP | HTTP/HTTPS | Installer node | {HubNameStart} | Allows installer node to push the execution environment image to {HubName} when using the bundle installer. | Fixed value
| 80/443 | TCP | HTTP/HTTPS | {EDAName} node | {HubNameStart} | | Fixed value
| 80/443 | TCP | HTTP/HTTPS | {EDAName} node | {ControllerNameStart} |  | Fixed value
| 80/443 | TCP | HTTP/HTTPS | {ControllerNameStart} | {HubNameStart} |  | Fixed value
| 80/443 | TCP | HTTP/HTTPS | Execution node | {HubNameStart} | Allows execution nodes to pull the execution environment image from {HubName}. | Fixed value
//| 443 | TCP | HTTPS | Controller node | Client | Web UI/API

//This exposes the mesh ingress receptor entry point for inbound connections.| `nginx_https_port`
| 80/443 | TCP | HTTP/HTTPS | Controller node | {OCPShort} |  Only required when using container groups to run jobs. | Host name of OpenShift API server
| 80/443 | TCP | HTTP/HTTPS | HA Proxy load balancer/Ingress Node | {GatewayStart} |  This is the ingress above the gateway that is customer controlled and can load balance requests to multiple gateways. | This port is customer managed outside of {PlatformNameShort}.
| 80/443 | TCP | HTTP/HTTPS | {GatewayStart} | {ControllerNameStart} |  | 
| 80/443 | TCP | HTTP/HTTPS | {GatewayStart} | {HubNameStart} |  | 
| 80/443 | TCP | HTTP/HTTPS | {GatewayStart} | {EDAName} |  | 
| 80/443 | TCP | HTTP/HTTPS | Receptor | Execution node | OCP Mesh ingress |
| 80/443 | TCP | HTTP/HTTPS | Receptor | Hop node | OCP Mesh ingress | 
| 80/443 | TCP | HTTP/HTTPS | {ControllerNameStart} |  HA Proxy load balancer/Ingress Node | | `automationgateway_main_url` 
| 80/443 | TCP | HTTP/HTTPS | {HubNameStart} |  HA Proxy load balancer/Ingress Node | | `automationgatweway_main_url` 
| 80/443 | TCP | HTTP/HTTPS | {EDAName} |  HA Proxy load balancer/Ingress Node | | `automationgateway_main_url` 
| 5432 | TCP | PostgreSQL | Controller node | PostgreSQL database | Open only if the internal database is used along with another component. Otherwise, this port should not be open. | `automationcontroller_pg_port`
| 5432 | TCP | PostgreSQL | {EDAName} node | PostgreSQL database | Open only if the internal database is used along with another component. Otherwise, this port should not be open. | `automationedacontroller_pg_port`
| 5432 | TCP | PostgreSQL | {HubNameStart} | PostgreSQL database | Open only if the internal database is used along with another component. Otherwise, this port should not be open. | `automationhub_pg_port`
| 5432 | TCP | PostgreSQL | {GatewayStart} | External database | Open only if the internal database is used along with another component. Otherwise, this port should not be open. | `automationgateway_pg_port`
| 6379 | TCP | PostgreSQL | {EDAName} | Redis node |  | 
| 6379 | TCP | PostgreSQL | {GatewayStart} | Redis node |  | 
| 8443 | TCP | HTTPS | {GatewayStart} | {GatewayStart} | nginx | 
| 16379 | TCP | Redis | Redis nodes | Redis nodes | Redis cluster bus port for a resilient Redis configuration | 
| 27199 | TCP | Receptor | Controller node | Execution node | Configurable

Mesh nodes directly peered to controllers. 

Direct nodes involved. 
27199 communication can be both ways (depending on installation inventory) for execution nodes
| `receptor_listener_port`

`peers`
| 27199 | TCP | Receptor | Controller node | Hop node | Configurable

ENABLE connections from hop nodes to Receptor port if relayed through hop nodes. | `receptor_listener_port`

`peers`
| 27199 | TCP | Receptor | Controller node | Hybrid node | Configurable

ENABLE connections from controllers to Receptor port if relayed through non-hop connected nodes. | `receptor_listener_port`

`peers`
| 27199 | TCP | Receptor | Execution node | Hop node | Configurable

Mesh 27199 communication can be both ways (depending on installation inventory) for execution nodes

ALLOW connection from controller(s) to Receptor port |
`receptor_listener_port`

`peers`
| 27199 | TCP | Receptor | Hop node | Execution node |  | `receptor_listener_port`

`peers`
| 27199 | TCP | Receptor | Execution node | Controller node | Configurable

Mesh 27199 communication can be both ways (depending on installation inventory) for execution nodes

ALLOW connection from controller(s) to Receptor port |
`receptor_listener_port`

`peers`
| 27199 | TCP | Receptor | OCP cluster | Execution node |  | 
| 50051 | TCP | GRPC | {GatewayStart} | {GatewayStart} |  | 
|===

[NOTE]
====
* Hybrid nodes act as a combination of control and execution nodes, and therefore Hybrid nodes share the connections of both. 

* If `receptor_listener_port` is defined, the machine also requires an available open port on which to establish inbound TCP connections, for example, 27199.

* It might be the case that some servers do not listen on receptor port (the default is 27199)
+
Suppose you have a  Control plane with nodes A, B, C, D
+
The RPM installer creates a strongly connected peering between the control plane nodes with a least privileged approach and opens the tcp listener only on those nodes where it is required. All the receptor connections are bidirectional, so once the connection is created, the receptor can communicate in both directions. 
+
The following is an example peering set up for three controller nodes:
+
Controller node A --> Controller node B
+
Controller node A --> Controller node C
+
Controller node B --> Controller node C
+
You can force the listener by setting
+
`receptor_listener=True`
+
However, a connection Controller B --> A is likely to be rejected as that connection already exists.
+
This means that nothing connects to Controller A as Controller A is creating the connections to the other nodes, and the following command does not return anything on Controller A:
+
`[root@controller1 ~]# ss -ntlp | grep 27199 [root@controller1 ~]#`
==== 

.{InsightsName}
[options="header"]
|===
|URL |Required for
|link:https://api.access.redhat.com[https://api.access.redhat.com:443] |General account services, subscriptions
|link:https://cert-api.access.redhat.com[https://cert-api.access.redhat.com:443] |Insights data upload
|link:https://cert.console.redhat.com[https://cert.console.redhat.com:443] |Inventory upload and Cloud Connector connection
|link:https://{Console}[https://console.redhat.com:443] |Access to Insights dashboard
|===

.Automation Hub
[options="header"]
|===
|URL |Required for
|link:https://console.redhat.com[https://console.redhat.com:443] |General account services, subscriptions
|link:https://catalog.redhat.com[https://catalog.redhat.com:443] |Indexing execution environments
|link:https://sso.redhat.com[https://sso.redhat.com:443] |TCP
|\https://automation-hub-prd.s3.amazonaws.com +
\https://automation-hub-prd.s3.us-east-2.amazonaws.com| Firewall access
|link:https://galaxy.ansible.com[https://galaxy.ansible.com:443] |Ansible Community curated Ansible content
|\https://ansible-galaxy-ng.s3.dualstack.us-east-1.amazonaws.com | Dual Stack IPv6 endpoint for Community curated Ansible content repository
|link:https://registry.redhat.io[https://registry.redhat.io:443] |Access to container images provided by Red Hat and partners
|link:https://cert.console.redhat.com[https://cert.console.redhat.com:443] |Red Hat and partner curated Ansible Collections
|===

.Execution Environments (EE)
[options="header"]
|===
|URL |Required for
|link:https://registry.redhat.io[https://registry.redhat.io:443] |Access to container images provided by Red Hat and partners
|`cdn.quay.io:443` | Access to container images provided by Red Hat and partners
|`cdn01.quay.io:443` | Access to container images provided by Red Hat and partners
|`cdn02.quay.io:443` | Access to container images provided by Red Hat and partners
|`cdn03.quay.io:443` | Access to container images provided by Red Hat and partners
|===

[IMPORTANT]
====
Image manifests and filesystem blobs are served directly from `registry.redhat.io`.
However, from 1 May 2023, filesystem blobs are served from `quay.io` instead.
To avoid problems pulling container images, you must enable outbound connections to the listed `quay.io` hostnames.

This change should be made to any firewall configuration that specifically enables outbound connections to `registry.redhat.io`.

Use the hostnames instead of IP addresses when configuring firewall rules.

After making this change, you can continue to pull images from `registry.redhat.io`.
You do not require a `quay.io` login, or need to interact with the `quay.io` registry directly in any way to continue pulling Red Hat container images.

For more information, see link:https://access.redhat.com/articles/6999582[Firewall changes for container image pulls].
====
