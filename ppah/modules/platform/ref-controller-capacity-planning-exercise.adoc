[id="ref-controller-capacity-planning-exercise"]

= Example capacity planning exercise

Determining the number and size of instances to support the desired workload must take the following into account:

* Managed hosts
* Tasks per hour per host
* Maximum number of concurrent jobs you want to support
* Maximum number of forks set on jobs
* Node size you prefer to deploy (CPU/Memory/Disk)

With this data, you can calculate the number of tasks per hour, which the cluster needs control capacity to process; as well as the number of "forks" or capacity you need to be able to run your peak load, which the cluster needs execution capacity to run.

For example, to plan for a cluster with:

* 300 managed hosts
* 1,000 tasks per hour per host, or 16 tasks per minute per host
* 10 concurrent jobs
* Forks set to five on playbooks
* Average event size 1 Mb
* Preferred node size of 4 cpu and 16GB Ram with disks rated at 3000 IOPs

Known factors:

* To run the 10 concurrent jobs, you must have at least (10 jobs * 5 forks) + (10 jobs * 1 base task impact of a job) = 60 execution capacity
* To control 10 concurrent jobs, you must have at least 10 control capacity.
* Running 1000 tasks * 300 managed hosts per hour produces at least 300,000 events per hour. 
+
You might have to run the job to see exactly how many events it produces because this is dependent on the specific task and verbosity. 
+
For example, a debug task printing "Hello World" produces six job events with the verbosity of `1` on one host. 
With a verbosity of `3`, it produces 34 job events on one host. 
Therefore, the task produces at least six events. 
That means closer to 3,000,000 events per hour or approximately 833 events per second.

To determine how many execution and control nodes you require, refer to the experiment results in the following table that show the observed event processing rate of a single control node with five execution nodes of equal size (API Capacity column). 

The default "forks" setting of job templates is five, so using this default, the maximum number of jobs a control node can dispatch to execution nodes makes five execution nodes of equal CPU/RAM use 100% of their capacity, arriving at the previously mentioned 1:5 ratio of control to execution capacity.

[cols="26%,15%,12%,12%,12%,12%,12%",options="header",]
|===
| Node | API Capacity | Default Execution Capacity | Default Control Capacity | Mean Event Processing Rate at 100% capacity usage | Mean Events
Processing Rate at 50% capacity usage | Mean Events Processing Rate at 40% capacity usage
| 4 CPU @ 2.5Ghz, 16 GB RAM Control Node, max 3000 IOPs disk | 100 - 300 requests/second | n/a | 137 jobs | 1100/second | 1400/second | 1630/second
| 4 CPU @ 2.5Ghz, 16 GB RAM Execution Node, max 3000 IOPs disk | n/a | 137 | 0 | n/a | n/a | n/a
| 4 CPU @ 2.5Ghz, 16 GB RAM DB Node, max 3000 IOPs disk | n/a | n/a | n/a | n/a | n/a | n/a
|===

This table shows that controlling jobs competes with job event processing on the control node. 
Therefore, over-provisioning control capacity can have a positive impact on reducing processing times. 
When processing times are high, users can experience a delay between when the job runs and when they can view the output in the API or UI.

For the example workload on 300 managed hosts, executing 1000 tasks per hour per host, 10 concurrent jobs with forks set to 5 on playbooks, and an average event size 1 Mb, do the following:

* Deploy 1 execution node, 1 control node, 1 DB node of 4 CPU @ 2.5Ghz, 16 GB RAM with disk having ~3000 IOPs.
* Keep default fork setting of 5 on job templates.
* Use the capacity adjustment feature on the control node to reduce the capacity down to 16 (lowest value) to reserve more of the control node's capacity for processing events.

image:perf-capacity-adj-instances.png[image]

.Additional Resources

For more information on workloads with high levels of API interaction, see link:https://www.ansible.com/blog/scaling-automation-controller-for-api-driven-workloads[Scaling Automation Controller for API Driven Workloads]. 

To summarize the guidance, best practices include:

* Use a load balancer
* Limit the rate
* Set max connections per controller to 100
* Use dynamic inventory sources instead of individually creating inventory hosts using the API
* Use webhook notifications instead of polling for job status

Since the published blog, additional observations have been made in the field regarding authentication methods. 
For automation clients that make many requests in rapid succession, using tokens is a best practice, because, depending on the type of user, there can be additional overheads when using basic authentication. 
For example, LDAP users using basic authentication trigger a process to reconcile if the LDAP user is correctly mapped to particular organizations, teams and roles. 